# Destination Deep Space 2019## Software Tasks### Vision Processing ApproachesSee [Approaches](https://wpilib.screenstepslive.com/s/currentCS/m/vision/l/682117-strategies-for-vision-programming).The following are different approaches to use Vision Processing.  The goal for each series of steps is to make the co-ordinates of the a "target" available on the RoboRio for navigation.1. RoboRio (OnBoard)    1. Initialize and capture Images    2. OpenCV Target Recognition2. PixyMon    1. Offline target training    2. Low level communication protocol (Serial IO format)    3. Command to PixyMon to start target recognition    4. Read target coordinates from PixyMon    5. Publish target coordinates to NetTable3. Raspberry Pi Camera (Java and/or Python)    1. Initialize and capture Images    2. OpenCV Target Recognition    3. Publish target coordinates to NetTable4. OffBoard co-processing (PC or Raspberry Pi)    1. Read Image from RoboRio video stream    2. OpenCV Target Recognition    3. Publish target coordinates to NetTable#### OpenCV Target RecognitionSee [GRIP](https://wpilib.screenstepslive.com/s/currentCS/m/vision/l/463566-introduction-to-grip)The GRIP tools provided can be used to test pipelines and generate code which can be easily ported for use in the approaches mentioned above.  Some additional logic may need to be written to handle certain cases.1. OpenCV processing to find contours2. Filter contours to find game targets#### Raspberry Pi Communication with RoboRioFor Raspberry Pi co-processing to be useful, data must be sent to the RoboRio (and possibly RoboRio to Pi).  NetworkTables is probably the best approach although socket programming would also work.   - Java     - WPILib NT libraries exists but they rely on C library  This must be compiled on the Raspberry Pi and set up with the Java runtime- Python     - There is a native implementation [here](https://robotpy.readthedocs.io/en/latest/install/pynetworktables.html#install-pynetworktables) 